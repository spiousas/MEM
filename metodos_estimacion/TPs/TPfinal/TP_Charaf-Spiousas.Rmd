---
title: "Trabajo final - M茅todos de Estimaci贸n 2023"
author: "Jesica Charaf e Ignacio Spiousas"
date: '2023-07-08'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
```

# Presentaci贸n

La ditribuci贸n elegida para trabajar es la binomial negativa. Para el desarrollo utilizaremos la parametrizaci贸n con $r$ y $\mu$ que se muestra a continuaci贸n:

Sea $X \sim BN(r,\mu)$, entonces, su funci贸n de probabilidad puntual puede expresarse como:

$$Px(x) = \frac{\Gamma(x+r)}{x!\Gamma(r)} \left( \frac{r}{r+\mu} \right)^r \left( \frac{\mu}{r+\mu} \right)^x,$$
con $x=1,2,3,...$. 

Adem谩s, se tiene que: $\mathbb{E}(X)=\mu$ y $V(X)=\mu + \frac{\mu^2}{r}$.

# Estimadores de momentos para $r$ y $\mu$

## Obtenci贸n de los estimadores

Consideremos al conjunto de par谩metros $\theta = (r,\mu)$ y que $X_1,...,X_n \overset{iid}{\sim} BN(r,\mu)$. Como tenemos dos par谩metros, para obtener los estimadores de momentos tendremos que utilizar el primer y segundo momento.

### Primer momento

De plantear el primer momentos obtenemos:

$$
\begin{aligned}
\overline{X} &= \mathbb{E}_{\hat{\theta}}(X) \\
\overline{X} &= \hat{\mu}.
\end{aligned}
$$

### Segundo momento

Para el segundo momento tenemos que plantear:

$$
\frac{1}{n} \sum_{i=1}^n X_i^2 = \mathbb{E}_{\hat{\theta}}(X^2),
$$
Y para esto vamos a recordar que $V(X) = \mathbb{E}_{\hat{\theta}}(X^2) - (\mathbb{E}_{\hat{\theta}}(X))^2$, por lo tanto:

$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n X_i^2 &= \mathbb{E}_{\hat{\theta}}(X^2) \\
\frac{1}{n} \sum_{i=1}^n X_i^2 &= V(X) + (\mathbb{E}_{\hat{\theta}}(X))^2 \\
\frac{1}{n} \sum_{i=1}^n X_i^2 &= \hat{\mu} + \frac{\hat{\mu}^2}{\hat{r}} + \hat{\mu}^2.
\end{aligned}
$$

### Resumiendo

Entonces, el sistema de ecuaciones que tenemos que resolver para hallar $\hat{r}$ y $\hat{\mu}$ es:

$$
\begin{aligned}
\overline{X} &= \hat{\mu} \\
\frac{1}{n} \sum_{i=1}^n X_i^2 &= \hat{\mu} + \frac{\hat{\mu}^2}{\hat{r}} + \hat{\mu}^2.
\end{aligned}
$$

### El despeje

En el caso del estimador de $\hat{\mu}$, no hace falta despejar nado ya que queda directamente definido por el primer momento. Por otro lado, a $\hat{r}$ lo podemos despejar f谩cilmente del segundo momento reemplazando $\mu$ por $\overline{X}$:

$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n X_i^2  - \hat{\mu} - \hat{\mu}^2 &= \frac{\hat{\mu}^2}{\hat{r}} \\
\frac{1}{n} \sum_{i=1}^n X_i^2  - \overline{X} - \overline{X}^2 &= \frac{\overline{X}^2}{\hat{r}} \\ 
\hat{r} &= \frac{\overline{X}^2}{\frac{1}{n} \sum_{i=1}^n X_i^2  - \overline{X} - \overline{X}^2}
\end{aligned}
$$

Entonces los estimadores de momentos son:

$$
\begin{aligned}
\hat{\mu}_{mo} &= \overline{X}  \\
\hat{r}_{mo} &= \frac{\overline{X}^2}{\frac{1}{n} \sum_{i=1}^n X_i^2  - \overline{X} - \overline{X}^2}
\end{aligned}
$$

## Consistencia

De nuevo, probar la consistencia del estimador de $\mu_{mo}$ es directo, ya que por LGN $\overline{X} \overset{c.s.}{\rightarrow}\mu$, entonces, $\hat{\mu}_{mo}$ es una estimador consistente para $\mu$.

Para estudiar la consistencia de $\hat{r}_{mo}$ tambi茅n vamos a utilizae LGN, pero vamos a tener que utilizar algunas propiedades. Por LGN sabemos que:

$$\sum_{i=1}^n X_i^2 \overset{c.s.}{\rightarrow} \mathbb{E}_{\hat{\theta}}(X^2) =  \mu + \frac{\mu^2}{r} + \mu^2.$$

Adem谩s, como $\overline{X} \overset{c.s.}{\rightarrow}\mu$ y $g(t)=t^2$ es una funci贸n continua, podemos afirmar que $\overline{X}^2 \overset{c.s.}{\rightarrow} \mu^2$. Luego

$$
\begin{aligned}
\frac{\overline{X}^2}{\frac{1}{n} \sum_{i=1}^n X_i^2  - \overline{X} - \overline{X}^2} &\overset{c.s.}{\rightarrow} \frac{\mu^2}{\mu + \frac{\mu^2}{r} + \mu^2 - \mu - \mu^2} \\
\frac{\overline{X}^2}{\frac{1}{n} \sum_{i=1}^n X_i^2  - \overline{X} - \overline{X}^2} &\overset{c.s.}{\rightarrow} \frac{\mu^2}{\frac{\mu^2}{r}} = r.
\end{aligned}
$$

Por lo tanto, $\hat{r}_{mo}$ es un estimador consistente para $r$.

## Distribuci贸n asint贸tica

Para la distribuci贸n asint贸tica vamos a generar simulaciones de datos con distribuci贸n binomial negativa y par谩metros poblacionales $r = 1$ y $\mu = 5$ y para cuatro tama帽os de muestra, con $n={10, 50, 100, 500}$. Con cada simulaci贸n vamos a estimar los par谩metros utilizando los estimadores de momentos ($\hat{\theta}$) y, a partir de ellos vamos a calcular los estimadores estandarizados de acuerdo a:

$$
\frac{\hat{\theta}-\theta}{se(\hat{\theta})}.
$$

Una vez obtenidos los valores estandarizados para $\hat{r}$ y $\hat{\mu}$ graficaremos los histogramas de ambas distribuciones comparados con la densidad de una distribuci贸n $\mathcal{N}(0,1)$.

Primero defino las funciones que me generar谩n mis estimaciones de momentos

```{r}
mu_mo <- function(x) {
  mean(x)
}

r_mo <- function(x) {
  (mean(x))^2/(mean(x^2) - mean(x) - mean(x)^2)
}
```

Luego, voy a realizar `Nrep` simulaciones del vector aleatorio $X$ con $\{X_i, ... , X_n\} \sim BN(\mu=5,r=1)$:

```{r simulacion MO}
ns <- c(1e1, 5e1, 1e2, 5e2)
Nrep <- 1e4
mu_pob <- 5
r_pob <- 1

est_rs <- vector(length = length(ns)*Nrep)
est_mus <- vector(length = length(ns)*Nrep)
est_ns <- vector(length = length(ns)*Nrep)

set.seed(12)
for (i in 1:length(ns)) {
  for (j in 1:Nrep) {
    data <- rnbinom(n = ns[i], size = r_pob, mu = mu_pob)
    est_ns[(i-1)*Nrep+ j] <- ns[i]
    est_mus[(i-1)*Nrep + j] <- mu_mo(data)
    est_rs[(i-1)*Nrep + j] <- r_mo(data)
  }
}
```

Luego estandarizamos los estimadores obtenidos con las muestra simuladas:

```{r}
sim_mo <- tibble(n = est_ns,
                 mu = est_mus,
                 r = est_rs) %>%
  filter(r!=Inf) %>%
  group_by(n) %>%
  mutate(mu_est = (mu - mu_pob)/sd(mu),
         r_est = (r - r_pob)/sd(r))
```

Y, finalmente, vemos los histogramas de estos estimadores estandarizados comparados con una distribuci贸n $\mathcal{N}(0,1)$. Primero $\hat{\mu}_{mo,est}$:

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=2, fig.align='center'}
sim_mo %>%
  ggplot() +
  geom_histogram(aes(x = mu_est, y = ..density..), alpha = .5, bins = 30 ) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "red", linewidth = 1) +
  scale_x_continuous(limits = c(-3,3)) +
  facet_grid(.~n, labeller = label_both) +
  labs(x = "mu de momentos estandarizado (Nrep = 10000)",
       y = "densidad") +
  theme_bw()
```

En los histogramas puede verse que, a煤n para valores chicos de $n$, la distribuci贸n del estimador estandarizado $\hat{\mu}_{mo,est}$ es muy similar a la l铆nea roja, es decir, su distribuci贸n asint贸tica es una $\mathcal{N}(0,1)$. Por otro lado, algo que podemos observar es que el estimador no pareciera tener un sesgo para ning煤n valor de $n$.

Y para $\hat{r}_{mo,est}$:

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=2, fig.align='center'}
sim_mo %>%
  ggplot() +
  geom_histogram(aes(x = r_est, y = ..density..), alpha = .5, bins = 30) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "red", linewidth = 1) +
  scale_x_continuous(limits = c(-3,3)) +
  ggh4x::facet_grid2(. ~ n, scales = "free_y", independent = "y") +
  labs(x = "r de momentos estandarizado (Nrep = 10000)",
       y = "densidad") +
  theme_bw()
```

Y en este caso, y al contrario que para $\hat{\mu}_{mo,est}$, vemos que s贸lo para valores de $n$ grandes la distribuci贸n de $\hat{r}_{mo,est}$ se aproxima a la $\mathcal{N}(0,1)$, mientras que para valores chicos de $n$ muestra una distribuci贸n m谩s concentrada y sesgada hacia la derecha.

Una cosa extra que podemos hacer, ya que tenemos simulaciones, es verificar la consistencia calculando el error cuadr谩tico medio emp铆rico (ECME):

```{r}
sim_mo %>% 
  group_by(n) %>%
  summarise(ECME_r = mean(r - r_pob)^2,
            ECME_mu = mean(mu - mu_pob)^2) %>%
  knitr::kable()
```

Puede verse que ambos estimadores son consistentes aunque, como se ve en los histogramas, $\hat{\mu}_{mo}$ parece ser un estimador insesgado mientras que $\hat{r}_{mo}$ parece ser un estimador asint贸ticamente insesgado, pero con un sesgo a derecha para valores chicos de n.

# Estimadores de m谩xima verosimilitud para $r$ y $\mu$

Para hallar los estimadores de m谩xima versosimilitud utilizaremos el comando `fitdistr` del paquete {MASS}. Al igual que hicimos con los estimadores de momentos, vamos 

## Obtenci贸n de los estimadores

A modo de ejemplo obtendremos los par谩metros $\hat{r}_{mv}$ y $\hat{\mu}_{mv}$ a partir de una muestra aleatoria de variables iid distribuidas como binomial negativa con par谩metros $r=1$ y $\mu=10$.

```{r}
mu_mo <- function(x) {
  fitdistr(x, densfun = "negative binomial")[2]
}

r_mo <- function(x) {
  fitdistr(x, densfun = "negative binomial")[1]
}
```

Puedo verirficar que las estimaciones me den bien:

## Consistencia

Para la consistencia vamos a repetir el procedimiento utilizado para la distribuci贸n asint贸tica de los estimadores de momentos pero calcularemos el ECME. Primero hagamos la simulaci贸n:

```{r simulacion EMV, warning=FALSE}
ns <- c(1e1, 5e1, 1e2, 5e2)
Nrep <- 1e4
mu_pob <- 5
r_pob <- 1

est_rs <- vector(length = length(ns)*Nrep)
est_mus <- vector(length = length(ns)*Nrep)
est_ns <- vector(length = length(ns)*Nrep)

set.seed(12)
for (i in 1:length(ns)) {
  cat(paste("n =", ns[i], "\n"))
  for (j in 1:Nrep) {
    data <- rnbinom(n = ns[i], size = r_pob, mu = mu_pob)
    est_mv <- fitdistr(data, densfun = "negative binomial")
    est_ns[(i-1)*Nrep+ j] <- ns[i]
    est_mus[(i-1)*Nrep + j] <- est_mv$estimate[2]
    est_rs[(i-1)*Nrep + j] <- est_mv$estimate[1]
  }
}
```

Luego, estandarizamos los estimadores obtenidos con las muestra simuladas:

```{r}
sim_mv <- tibble(n = est_ns,
                 mu = est_mus,
                 r = est_rs) %>%
  filter(r!=Inf) %>%
  group_by(n) %>%
  mutate(mu_est = (mu - mu_pob)/sd(mu),
         r_est = (r - r_pob)/sd(r))
```

Y, por 煤ltimo, calculamos el ECME:

```{r}
sim_mv %>% 
  group_by(n) %>%
  summarise(ECME_r = mean(r - r_pob)^2,
            ECME_mu = mean(mu - mu_pob)^2) %>%
  knitr::kable()
```

De la misma forma que en los estimadores de momentos, pareciera que, si bien ambos son consistentes, $\mu_{mv}$ es insesgado mientras que $r_{mv}$ es asint贸ticamente insesgado.

## Distribuci贸n asint贸tica

Despu茅s de estandarizar podemos ver los histogramas para $\hat{\mu}_{mv,est}$:
```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=2, fig.align='center'}
sim_mv %>%
  ggplot() +
  geom_histogram(aes(x = mu_est, y = ..density..), alpha = .5, bins = 30 ) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "red", linewidth = 1) +
  scale_x_continuous(limits = c(-3,3)) +
  facet_grid(.~n, labeller = label_both) +
  labs(x = "mu de momentos estandarizado (Nrep = 10000)",
       y = "densidad") +
  theme_bw()
```

, y para $\hat{r}_{mv,est}$:

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=2, fig.align='center'}
sim_mv %>%
  ggplot() +
  geom_histogram(aes(x = r_est, y = ..density..), alpha = .5, bins = 30) + 
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "red", linewidth = 1) +
  scale_x_continuous(limits = c(-3,3)) +
  ggh4x::facet_grid2(. ~ n, scales = "free_y", independent = "y") +
  labs(x = "r de momentos estandarizado (Nrep = 10000)",
       y = "densidad") +
  theme_bw()
```

En los que, al igual que para los estimadores de momentos se ve que la distribuci贸n asint贸tica para ambos par谩metros es una $\mathcal{N}(0,1)$, pero con diferencias para valores de $n$ peque帽os.

# Aplicaciones y datos

La principal aplicaci贸n de este tipo de distribuciones en el modelado estad铆stico es cuando tenemos una variable de cuenta en la que la varianza es m谩s grande que la media, esto se llama que tenemos agregaci贸n. 

Por ejemplo, supongamos que tenemos 
<!-- In the context of ecology and biodiversity, a more relevant parametrisation and interpretation of the negative binomial distribution relates to the counting process of some random phenomena (e.g., occurrences of plants or animals, abundance of species). This is achieved by reparametrising Equation (1) in terms of enemos una vits mean  -->
<!--  and a dispersion or aggregation index  -->
<!--  governing the count variation. Specifically, let =/(+) -->
<!-- , where = -->
<!-- . Then, we can write Equation (1) as -->
